{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa74fe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-03T15:53:14+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-03T15:53:14+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'sample_document.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Data Ingestion and Processing Guide\\nIntroduction\\nData ingestion is a fundamental process in modern data analytics and machine\\nlearning workflows. It involves collecting, importing, and processing data from various\\nsources for immediate use or storage in databases. This document provides an\\noverview of key concepts, best practices, and implementation strategies for effective\\ndata ingestion.\\nKey Concepts\\n1. Data Sources: Data can originate from multiple sources including databases, APIs,\\nfile systems, streaming platforms, and external services. Each source requires\\nspecific handling methods and protocols. 2. Data Formats: Common formats include\\nJSON, CSV, XML, Parquet, and Avro. Understanding format characteristics helps in\\nchoosing appropriate processing tools. 3. Data Quality: Ensuring data accuracy,\\ncompleteness, and consistency is crucial for reliable analytics and decision-making\\nprocesses. 4. Scalability: Data ingestion systems must handle increasing data\\nvolumes while maintaining performance and reliability.\\nBest Practices\\n\\x7f Data Validation: Implement comprehensive validation rules to ensure data quality \\x7f\\nError Handling: Design robust error handling mechanisms for failed ingestion\\nattempts \\x7f Monitoring: Set up monitoring and alerting for ingestion pipelines \\x7f\\nDocumentation: Maintain clear documentation of data sources, schemas, and\\nprocesses \\x7f Security: Implement appropriate security measures for sensitive data \\x7f\\nBackup Strategies: Establish reliable backup and recovery procedures'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-03T15:53:14+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-03T15:53:14+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'sample_document.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content=\"Implementation Strategies\\nBatch Processing: Suitable for large datasets that don't require real-time processing.\\nExamples include daily ETL jobs and bulk data imports. Stream Processing: Ideal for\\nreal-time data ingestion where low latency is critical. Technologies like Apache Kafka\\nand Apache Flink are commonly used. Hybrid Approaches: Combining batch and\\nstream processing for optimal performance and cost-effectiveness.\\nConclusion\\nEffective data ingestion is the foundation of successful data-driven organizations. By\\nfollowing best practices and choosing appropriate technologies, organizations can\\nbuild robust, scalable, and maintainable data ingestion pipelines that support their\\nanalytical and operational needs.\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"sample_document.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f256bc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-03T15:53:14+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-03T15:53:14+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'sample_document.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='volumes while maintaining performance and reliability.\\nBest Practices\\n\\x7f Data Validation: Implement comprehensive validation rules to ensure data quality \\x7f\\nError Handling: Design robust error handling mechanisms for failed ingestion\\nattempts \\x7f Monitoring: Set up monitoring and alerting for ingestion pipelines \\x7f\\nDocumentation: Maintain clear documentation of data sources, schemas, and\\nprocesses \\x7f Security: Implement appropriate security measures for sensitive data \\x7f\\nBackup Strategies: Establish reliable backup and recovery procedures')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "final_documents[0]\n",
    "final_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9d1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
